2025-03-08 12:41:40,570 INFO Starting process_data_all: Counting ALL unique words
2025-03-08 12:41:46,817 INFO Spark session initialized successfully for process_data_all.
2025-03-08 12:41:46,817 INFO (ALL) Reading data from input path: dataset/test.jsonl
2025-03-08 12:41:54,231 INFO (ALL) Data read into Spark DataFrame successfully.
2025-03-08 12:41:56,237 INFO (ALL) DataFrame Schema: struct<description:string,label:bigint,title:string>
2025-03-08 12:41:56,239 INFO (ALL) Number of records in input DataFrame: 7600
2025-03-08 12:41:56,240 INFO (ALL) Tokenization the 'description' column into individual words and cleaning punctuation/special chars.
2025-03-08 12:41:58,232 INFO (ALL) Number of total words (rows) after cleaning: 232197
2025-03-08 12:41:58,236 INFO (ALL) Grouping by word and counting occurrences for all unique words.
2025-03-08 12:41:58,346 INFO (ALL) Temporary output path: ztmp/data/temp_word_count_all_20250308
2025-03-08 12:41:58,346 INFO (ALL) Final output path: ztmp/data/word_count_all_20250308.parquet
2025-03-08 12:41:58,347 INFO (ALL) Saving word counts to temp Parquet directory.
2025-03-08 12:42:03,717 INFO (ALL) Validating part files in the temporary directory.
2025-03-08 12:42:03,719 INFO (ALL) Part files found: ['ztmp/data/temp_word_count_all_20250308\\part-00000-dd82ced5-c182-48ba-a269-ae7457aa6d32-c000.snappy.parquet']
2025-03-08 12:42:03,720 INFO (ALL) Moving and renaming the part file to the final destination.
2025-03-08 12:42:03,722 INFO (ALL) Removing temporary directory: ztmp/data/temp_word_count_all_20250308
2025-03-08 12:42:03,725 INFO (ALL) Final Parquet file saved at: ztmp/data/word_count_all_20250308.parquet
2025-03-08 12:42:03,727 INFO (ALL) Reading the final Parquet file for verification.
2025-03-08 12:42:04,544 INFO (ALL) Number of unique words in final result: 23482
2025-03-08 12:42:05,030 INFO process_data_all: Counting ALL unique words completed successfully.
2025-03-08 12:42:05,533 INFO (ALL) Spark session stopped for process_data_all.

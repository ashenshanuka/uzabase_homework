2025-03-08 12:41:19,984 INFO Starting process_data: Counting specified target words
2025-03-08 12:41:25,427 INFO Spark session initialized successfully for process_data.
2025-03-08 12:41:25,427 INFO Reading data from input path: dataset/test.jsonl
2025-03-08 12:41:31,343 INFO Data read into Spark DataFrame successfully for process_data.
2025-03-08 12:41:32,754 INFO DataFrame Schema: struct<description:string,label:bigint,title:string>
2025-03-08 12:41:32,755 INFO Number of records in input DataFrame: 7600
2025-03-08 12:41:32,755 INFO Target words: ['president', 'the', 'asia']
2025-03-08 12:41:32,756 INFO Tokenization the 'description' column into individual words and filtering on target words.
2025-03-08 12:41:33,982 INFO Number of rows after applying target word filter: 12811
2025-03-08 12:41:33,984 INFO Grouping by targeted word and counting occurrences
2025-03-08 12:41:34,038 INFO Temporary output path: ztmp/data/temp_word_count_20250308
2025-03-08 12:41:34,038 INFO Final output path: ztmp/data/word_count_20250308.parquet
2025-03-08 12:41:34,038 INFO Saving word counts to temp Parquet directory.
2025-03-08 12:41:36,587 INFO Looking for part-* files in the temporary directory.
2025-03-08 12:41:36,590 INFO Part files found: ['ztmp/data/temp_word_count_20250308\\part-00000-abb63c8b-7df0-4e81-8157-d5dd63b6556c-c000.snappy.parquet']
2025-03-08 12:41:36,596 INFO Moving and renaming the part file to final destination.
2025-03-08 12:41:36,601 INFO Removing temporary directory: ztmp/data/temp_word_count_20250308
2025-03-08 12:41:36,609 INFO Final Parquet file saved at: ztmp/data/word_count_20250308.parquet
2025-03-08 12:41:36,609 INFO Reading the final Parquet file for verification.
2025-03-08 12:41:37,256 INFO Number of distinct words in final result: 3
2025-03-08 12:41:37,610 INFO process_data: Counting target words completed successfully.
2025-03-08 12:41:37,960 INFO Spark session stopped for process_data.
